# -*- coding: utf-8 -*-
"""Klasifikasi_wine_vinho_verde.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z3RnOKp6Z6nZ5alb3JuK4pywTdBcMeHk

Laporan Proyek Machine Learning - Moh. Novil Ma'arij

Domain Proyek : Pemasaran/Ekonomi

# **Klasifikasi kualitas wine Vinho Verde**

![gambar wine vinho verde](https://static.independent.co.uk/2023/07/13/11/vintages-buy-43784e0e-11dc-11ee-9aaf-3ad6dbe6fd05.jpg?quality=75&width=1250&crop=3%3A2%2Csmart&auto=webp)

## **Latar Belakang**

Region Vinho Verde di Portugal telah lama dikenal sebagai produsen anggur yang unik dengan karakter rasa yang segar, asam, dan sedikit bergelembung (frizzante). Terdapat dua jenis wine Vinho Verde, yaitu white dan red, yang masing-masing memiliki varietas tersendiri.

Seiring dengan perubahan pola pasar dan ketatnya persaingan, konsumen kini tidak hanya mencari wine yang enak, tetapi juga wine dengan kualitas terbaik.

Menyikapi hal ini, produsen wine Vinho Verde selama ini melakukan klasifikasi kualitas wine secara tradisional, seperti menggunakan metode sensorik dan teknik kimiawi. Kedua metode tersebut memberikan hasil yang baik, namun memiliki kelemahan yaitu tingkat human error yang cukup tinggi.

Oleh karena itu, dalam proyek ini akan dikembangkan sebuah model machine learning untuk mengklasifikasikan kualitas wine Vinho Verde berdasarkan kadar kimiawinya.

## **Import Library**

Pada bagian ini di import library yang dbutuhkan, mulai dari librarary pemrosesan hingga library untuk model.
"""

# Import libraries
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

# Machine Learning libraries
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, f1_score
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.combine import SMOTETomek
from sklearn.model_selection import RandomizedSearchCV
from sklearn.preprocessing import LabelEncoder

"""# **Data Understanding**

pada bagian data Undestanding kita akan memahami struktur dari dataset, memahami label-labelnya, dan mengetahui masalah-masalah yang ada pada data sehingga dapat di tentukan nantinya metode apa yang harus di lakukan pada saat data preparation, pada proses ini juga nantinya di cari mana fitur yang memiliki korelasi tinggi

### **Data Loading**

koda ini di gunakan untuk mengunduh dataset yang berasal dari kaggle, untuk menjalankan kode ini pastikan `memiliki kode kredensial API` karena saat kode ini di jalankan dia akan meminta memasukkan API kaggle yang biasanya berbentuk JSON. Hasil dataset yang di unduh akan berbentuk .zip oleh karena itu perlu di unzip.
"""

# files.upload()

# !mkdir -p ~/.kaggle
# !cp kaggle.json ~/.kaggle/
# !chmod 600 ~/.kaggle/kaggle.json

# !kaggle datasets download -d rajyellow46/wine-quality
# !unzip wine-quality.zip

"""pada baris ini dataset yang telah di unzip di masukkan pada sebuah variabel `vinho_verde`, hal ini di lakukan untuk mempermudah proses pengolahan data"""

vinho_verde = pd.read_csv('winequalityN.csv')

"""### **Dataset Informations**

| Jenis | Keterangan |
|-------|------------|
|Nama   | Wine Quality|
|Sumber | [Kaggle](https://www.kaggle.com/datasets/rajyellow46/wine-quality)|
|Pengelola| [Raj Parmar](https://www.kaggle.com/rajyellow46)
|Lisensi| Tidak Spesifik|
|Visibilitas| Publik|
|Label| *Earth and Nature, Classification, Alcohol, Regression*|
|Kebergunaan| 7.6|

dataset ini memang diambil dari kaggle dan tidak tertulis linsensi yang jelas dari datasetnya, namun setelah di telusur lebih lanjut, dataset ini ternyata berasal dari dataset [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/dataset/186/wine+quality)

### **Variabel Description**
"""

vinho_verde.sample(3)

"""dari tabel di atas ternyata terdapat 13 label pada dataset, berikut penjelasan masing-masing labelnya:
1. `type` -> tipe wine
2. `fixed acidity`-> Asam tetap dalam anggur
3. `Volatile acidity` -> Asam yang mudah menguap
4. `Citric acid` -> Asam Sitrat
5. `Residual sugar` -> Gula sisa pasca fermentasi
6. `Chlorides` -> kandungan garam (Klorida)
7. `Free sulfur dioxide` -> SO₂ bebas yang berfungsi sebagai pengawet
8. `Total sulfur dioxide` -> Jumlah total SO₂ (bebas + terikat)
9. `Density` -> Masa jenis anggur
10. `pH` -> Tingkat keasaman
11. `Sulphates` -> Kandungan Sulfat
12. `Alcohol` -> Kandungan alkohol dalam persen volume
13. `Quality` -> Skor kualitas anggur  
"""

vinho_verde.info()

"""dari 13 label, 11 nya bertipe float dan 1 bertipe integer dan 1 bertipe object atau categorical, kemudian juga ada ketidak samaan jumlah dataset pada masing-masing label, hal ini mugkin di karenakan ada data NULL atau ada data duplikat."""

vinho_verde.describe()

"""dari tabel di atas kita mendapatkan beberapa informasi tambahan :
1. Kadar rata-rata fixed acidity adalah sekitar 7.22.
2. Residual sugar (gula tersisa) rata-rata 5.44, yang menunjukkan sebagian besar anggur memiliki kadar gula rendah sampai sedang.
3. Variabel residual sugar dan total sulfur dioxide memiliki standar deviasi cukup besar (4.76 dan 56.52), menunjukkan variasi nilai yang luas.
4. 50% (median) dari fixed acidity adalah 7.00, menunjukkan distribusi simetris mendekati rata-rata.
5. Median residual sugar adalah 3.00, lebih rendah dari rata-rata (5.44), artinya distribusi agak skew ke kanan (ada beberapa nilai tinggi ekstrim).
6. Fixed acidity maksimal 15.90, cukup jauh dari rata-rata, menunjukkan adanya beberapa anggur dengan kandungan asam tinggi.
7. Residual sugar maksimum sangat tinggi yaitu 65.80, kemungkinan ada anggur yang sangat manis.

### **Univariate Analysis**

Univariate analysis adalah tahapan dimana dilakukan proses analisa pada tiap-tiap variabel atau label dalam dataset secara terpisah. tujuannya adalah untuk :
1. memahami distribusi data
2. identifikasi nilai median
3. persebaran data

untuk melakukan univariate analysis kita akan menggunakan histogram. karena histogram efektif untuk data numerikal maka perlu di lakukan filtering untuk proses histogram hanya untuk data numerikal. kode di bawah menunjukkan sebuah vriabel `numerical_cols` yang hanya berisi kolom-kolom numerik.
"""

numerical_cols = vinho_verde.select_dtypes(include=[np.number]).columns

"""untuk menampilkan histogrma dari variabel `numerical_cols` maka digunakan library matplotlib, pada kode di bawah di lakukan iterasi pada setiap kolom dan kemudian di susun secara grid 2 kesamping"""

n = len(numerical_cols)
cols = 2  # jumlah kolom
rows = (n + cols - 1) // cols  # hitung baris supaya cukup

fig, axes = plt.subplots(rows, cols, figsize=(12, rows * 3))
axes = axes.flatten()  # supaya mudah indexing

for i, col in enumerate(numerical_cols):
    axes[i].hist(vinho_verde[col].dropna(), bins=30, edgecolor='black')
    axes[i].set_title(f'Distribusi Histogram: {col}')
    axes[i].set_xlabel(col)
    axes[i].set_ylabel('Frekuensi')

# Hilangkan axes kosong jika ada
for j in range(i+1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

"""Hasil dari histogram di atas menunjukkan:

1. `Fixed Acidity`-> cenderung normal dengan puncak frekuensi sekitar 6-7.
2. `Volatile Acidity` -> Distribusinya miring ke kanan, dengan nilai mayoritas volatile acidity rendah, sekitar 0.2-0.4, dan ada beberapa nilai tinggi yang jarang.
3. `Citric Acid` -> Sebagian besar nilai rendah, sekitar 0.2-0.4, dengan distribusi miring ke kanan dan banyak nilai kecil mendekati nol.
4. `Residual Sugar`-> didominasi oleh angka rendah (0-10), tapi ada beberapa nilai outlier yang cukup tinggi (hingga sekitar 60).
5. `Chlorides`-> sangat terkonsentrasi di bawah 0.1 dengan puncak frekuensi sangat tinggi pada nilai rendah.
6. `Free Sulfur Dioxide`-> berada di kisaran 10-50, dengan frekuensi menurun untuk nilai yang lebih tinggi.
7. `Total Sulfur Dioxide` -> relatif lebih merata, tapi tetap banyak nilai yang berkisar 100-200.
8. `Density`-> hampir seragam pada nilai sekitar 0.99-1.0 dengan sedikit variasi, mendekati distribusi normal dengan puncak di sekitar 0.995.
9. `pH` -> mendekati normal dengan nilai puncak sekitar 3.2-3.3.
10. `Sulphates` -> Mayoritas berkisar di 0.4-0.7, dengan distribusi miring ke kanan.
11. `Alcohol` -> tersebar cukup merata antara 9-12, dengan puncak frekuensi sekitar 9-10 dan 11.
12. `Quality` -> Distribusi kualitas anggur menunjukkan nilai paling banyak pada 5 dan 6, dengan sedikit nilai kualitas tinggi (7-8) dan rendah (3-4).

### **Multivariate Analysis**

setelah melakukan analisis pada tiap-tiap variabel selanjutnya akan di lakukan analisis pada 2 atau lebih variabel secara bersamaan (multivariate). pada kode di bawah di gunakan metode korelasi dengan fungsi `corr()` untuk mengecek korelasi dari semua variabel, teknik korelasi yang digunakan adalah pearson. hasil dari korelasinya kemudian di tampilkan dalam bentuk heatmap untuk mempermudah pembacaan.
"""

corr = vinho_verde[numerical_cols].corr()

# Visualisasi heatmap korelasi
plt.figure(figsize=(12,8))
sns.heatmap(corr, annot=True, fmt=".2f", cmap='coolwarm', square=True)
plt.title('Heatmap Korelasi Variabel Numerik')
plt.show()

"""dari kode heatmap di atas variabel yang memiliki korelasi tinggi adalah :
1. `residual sugar` dengan `total sulfur dioxide` mencapai `50`
2. `residual sugar` dengan `desnity` mencapai `55`
3. `free sulful dioxode` dengan `toal sulfur dioxide` mencapai `72`
4. `alcohol` dengan `desnity` memiliki hubungan negatif `-69`

# **Data Preparation**

setelah kita mengetahui struktur pada data dan masalah-masalah yang ada pada data selanjutnya adalah tahap pengolahan data agar siap di gunakan oleh model. secara umum tahap-tahap pada proses ini meliputi:
1. penghapusan data `NULL`
2. penghapusan data `duplikat`
3. penghapusan `outlier`

### **Remove Missing Values**

untuk memastikan keberadaan missing values atau disebut juga data null, maka digunaan fungsi `isnull()`. missing values pada data dapat menyebabkan bias hingga penurunan akurasi oleh karena itu penanganan missing values sangatlah penting
"""

vinho_verde.isnull().sum()

"""dari hasil di atas maka terdapat beberapa missing values pada variabel `fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, PH, `dan `sulphates`

untuk menangani missing values ada beberapa metode seperti inputasi atau penghapusan, berhubung missing values pada dataset ini sangatlah kecil sehingga penghapusan tidak akan mempengaruhi jumlah data maka, pengapusan di pilih
"""

vinho_verde = vinho_verde.dropna()

"""untuk memastikan missing values sudak tidak ada kita mengulangi code sebelumnya"""

vinho_verde.isnull().sum()

"""dari hasil output di atas sekarang sudah tidak ada lagi missing values pada semua variabel.

### **Remove Duplicates Values**

setela missing values sudah di hapus selanjutnya di lakukan pengecekan data duplikat, data duplikat dapat mempengaruhi akurasi model karena banyaknya data yang sama akan membut model menjadi overfit.

untuk mengetahui jumlah data duplikat di gunakan fungsi `duplicated()` seperti pada kode di bawah
"""

vinho_verde.duplicated().sum()

"""hasil pada output di atas terdapat 1168 data duplikat pada dataset. oleh karena itu kita perlu menghapusnya dengan menggunakan fungi `drop_duplicated()`"""

vinho_verde.drop_duplicates(inplace=True)

"""untuk memastikan data duplikat sudah terhapus maka kita ulangi kode sebelumnya."""

vinho_verde.duplicated().sum()

"""hasil dari output di atas menunjukkan bahwa tidak ada lagi data duplikat pada dataset

### **Find Multicollinearity**

multikolniearitas merupakan keadaan ketika 2 atau lebih variabel memiliki korelasi yang sangat erat, hal ini kemudian dapat menyebabkan model sulit mengidentifikasi variabel penting, koefisien regresi tidak stabil, standard error koefisien membesar dll.

untuk meneukan variabel multicolinearitas di buatlah sebuah fungsi `find_correlated_features()` pada fungsi ini setiap variabel akan di cek korelasinya dan akan di bandingkan dengan angka threshold, di sini defaultnya adalah 8 , jadi jika ada sebuah variabel yang korelasinya melebihi 8 maka akan di hitung sebagai variabel multikolinearitas. hasil dari fungsi ini akan di kembalikan dalam bentuk list.
"""

def find_correlated_features(corr_matrix, threshold=0.8):
    """Find pairs of highly correlated features"""
    high_corr_pairs = []
    for i in range(len(corr_matrix.columns)):
        for j in range(i+1, len(corr_matrix.columns)):
            if abs(corr_matrix.iloc[i, j]) > threshold:
                high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))
    return high_corr_pairs

"""setelah fungsi di buat, fungsi`find_correlated_features()` kemudian di panggil, namun dengan nilai threshold yang di set 7, hal ini di karenakan dari hasil multivariate sebelumnya korelasi paling tinggi adalah 72"""

corr_matrix = vinho_verde[numerical_cols].corr()
high_corr = find_correlated_features(corr_matrix, 0.7)
print(f"\nHighly correlated feature pairs (>0.7):")
for pair in high_corr:
    print(f"{pair[0]} - {pair[1]}: {pair[2]:.3f}")

"""seperti asumsi sebelumnya output menunjukkan free sulfur dioxide dan total sulfur dioxide adalah variabel dengan korelasi yang melebihi 7

### **Feature Engineering**

pada tahap ini, akan di buat variabel atau label baru yang bertujuan untuk membantu model lebih memahami pola yang ada pada data.
"""

# 1. Encode categorical variables
vinho_verde_processed = vinho_verde.copy()
vinho_verde_processed['type_encoded'] = vinho_verde_processed['type'].map({'white': 0, 'red': 1})

# 2. Create new features
vinho_verde_processed['total_acidity'] = vinho_verde_processed['fixed acidity'] + vinho_verde_processed['volatile acidity']
vinho_verde_processed['acid_to_sugar_ratio'] = vinho_verde_processed['total_acidity'] / (vinho_verde_processed['residual sugar'] + 1)
vinho_verde_processed['so2_ratio'] = vinho_verde_processed['free sulfur dioxide'] / (vinho_verde_processed['total sulfur dioxide'] + 1)
vinho_verde_processed['density_alcohol_interaction'] = vinho_verde_processed['density'] * vinho_verde_processed['alcohol']

"""Pada kode di atas dilakukan beberapa langkah feature engineering, yaitu:

1. Meng-`copy DataFrame` asli, DataFrame asli `vinho_verde` disalin ke variabel baru `vinho_verde_processed` agar proses pengolahan data tidak merubah data asli dan lebih aman dalam eksperimen.
2. Melakukan `encoding` pada variabel kategorikal `type`, Variabel type yang awalnya berupa data kategorikal (tipe wine: white dan red) diubah menjadi numerik biner (0 untuk white, 1 untuk red). Hal ini dilakukan karena model machine learning umumnya tidak dapat langsung memproses data berbentuk teks.
3. Menggabungkan variabel `fixed acidity` dan `volatile acidity`, Kedua variabel ini sama-sama mengukur tingkat keasaman pada wine, sehingga digabung menjadi variabel baru `total_acidity`. Dengan penggabungan ini, model mendapatkan gambaran menyeluruh tentang tingkat keasaman total, yang bisa lebih informatif dibandingkan melihat keduanya secara terpisah.
4. Membuat variabel rasio `acid_to_sugar_ratio`, Variabel baru ini adalah rasio antara `total_acidity` dengan `residual sugar` (ditambah 1 untuk menghindari pembagian nol).
Rasio ini menggambarkan keseimbangan antara asam dan gula dalam wine, yang merupakan faktor penting dalam menentukan rasa dan kualitas.
5. Membuat variabel rasio `so2_ratio`, Variabel ini merupakan rasio antara `free sulfur dioxide` dengan `total sulfur dioxide` (ditambah 1). Rasio ini menunjukkan proporsi sulfur dioksida bebas sebagai pengawet dibandingkan total kandungan sulfur dioksida, yang mempengaruhi daya tahan dan kualitas wine.
6. Membuat variabel interaksi `density_alcohol_interaction`, Variabel ini adalah hasil perkalian antara `density`dan `alcohol`. Tujuannya untuk menangkap efek gabungan dari kedua variabel tersebut terhadap kualitas wine, karena interaksi antara massa jenis dan kadar alkohol bisa memiliki pengaruh non-linear pada hasil.

### **Remove Outliers**

outliers merupakan sebuah data pada dataset dengan nilai yang sangat berbeda dengan nilai-nilai yang lain, ouliers bisa terbentuk karena human eror atau memang karena kondisi real yang terjadi di lapangan, jadi outliers ini tidak selalu berarti buruk.

adanya outliers pada data juga mengganggung proses pembelajaran model, ourliers dapat mengurangi kemampuan generalisasi model pada data.

untuk menampikan outliers maka digunakan visualisasi boxlpot seperti pada kode berikut
"""

plt.figure(figsize=(15, 12))
for i, col in enumerate(numerical_cols, 1):
    plt.subplot(4, 3, i)
    sns.boxplot(y=vinho_verde[col])
    plt.title(f'Boxplot {col}')
plt.tight_layout()
plt.show()

"""titik di luar dari garis atas (batas maksimum) dan garis bawah (batas minimum) adalah outlier, jadi pada output di atas terdapa outliers pada semua variabel, namun terdapat 3 variabel dengan jumlag outliers yang sangat sedikit, yaitu pada variabel `alcohol, density` dan `quality`.

untuk menghapus outliers, maka dibuatlah sebuah fungsi remove_outliers_iqr, pada fungsi ini outliers di hapus dengan menggunakan metode IQR, berikut adalah tahapan dari metode IQR:

1. Menghitung kuartil dan IQR untuk tiap kolom. Kuartil pertama (Q1) dan kuartil ketiga (Q3) dihitung sebagai batas bawah dan atas distribusi data tengah.
Selisih Q3 dan Q1 disebut IQR (Interquartile Range) yang merepresentasikan rentang nilai tengah 50% data.
2. Menentukan batas bawah dan batas atas untuk mendeteksi outlier. Batas bawah dihitung sebagai Q1 - factor * IQR, dan batas atas sebagai Q3 + factor * IQR.
Faktor 2.0 digunakan agar kriteria deteksi outlier lebih longgar dibanding standar umum 1.5, sehingga lebih sedikit data yang dianggap outlier.
3. Memfilter data untuk menghapus nilai yang di luar batas bawah dan atas. Baris data yang memiliki nilai kolom di luar rentang ini dianggap outlier dan dihapus dari DataFrame df_clean.
"""

def remove_outliers_iqr(df, columns, factor=2.0):  # Using factor=2.0 instead of 1.5 to be less aggressive
    """Remove outliers using IQR method"""
    df_clean = df.copy()

    for col in columns:
        Q1 = df_clean[col].quantile(0.25)
        Q3 = df_clean[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - factor * IQR
        upper_bound = Q3 + factor * IQR

        before_count = len(df_clean)
        df_clean = df_clean[(df_clean[col] >= lower_bound) & (df_clean[col] <= upper_bound)]
        after_count = len(df_clean)
        print(f"Removed {before_count - after_count} outliers from {col}")

    return df_clean

"""setelah fungsi remove_outliers_iqr di buat selanjutnya kode ini akan di implementasikan, namun kode ini hanya akan di implementasikan pada variabel selaint target yaitu quality, oleh karena itu di lakukan filtering. setelah di lakukan filtering fungsinya kemudian di terapkan pada semua variabel."""

numerical_features = [col for col in numerical_cols if col != 'quality']
print(f"Shape before outlier removal: {vinho_verde_processed.shape}")
vinho_verde_processed = remove_outliers_iqr(vinho_verde_processed, numerical_features)
print(f"Shape after outlier removal: {vinho_verde_processed.shape}")

"""hasil dari output di atas menunjukkan umlah dataset sebelum outliers di hapus yaitu 5295 data, kemudian di tampilkan jumlah data yang di hapus pada masing-masing variabel atau kolom, dan di akhir kemudian di tampilkan total data setelah melewati proses penghapus outlier menjadi 4285 jadi ada 1010 data yang di hapus pada dataset.

### **Remove High Correlation Feature**

setelah feature enginering tadi sempat menambahkan fitur atau variabel baru, dan sudah di temukan beberapa variabel berkorelasi tinggi, maka kita perlu menangani data-data berkorelasi tinggi ini. pertama data di pisah menjadi x dan y, dimana x adalah fitur dan y adalah target.
"""

# Prepare features and target
feature_columns = [col for col in vinho_verde_processed.columns if col not in ['quality', 'type']]
X = vinho_verde_processed[feature_columns]
y = vinho_verde_processed['quality']

print(f"Features used: {feature_columns}")

"""untuk menghapus variabel fitur dengan korelasi tinggi di buatlah fungsi remove_high_correlation, pada fungsi ini setiap fitur yang korelasinya melebihi 8 (nilai threshold) maka akan diangap sebagai matriks berkorelasi tinggi. setelah itu akan dipilih 3 variabel dengan korelasi tertinggi melebihi 8. nah 3 fitur teratas ini lah yang nantinya akan di hapus."""

def remove_high_correlation(X, threshold=0.8):
    """Remove highly correlated features"""
    corr_matrix = X.corr().abs()
    upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))

    to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > threshold)]
    print(f"Dropping highly correlated features: {to_drop}")

    return X.drop(columns=to_drop)

"""setelah fungsi di buat maka selanjutnya fungsinya di implementasikan seperti pada kode di bawah"""

X_reduced = remove_high_correlation(X, threshold=0.8)
print(f"Features after correlation removal: {X_reduced.shape[1]}")

"""output dari kode di atas menunjukkan 3 kolom yang di hapus karena memiliki korelasi yang sangat tinggi, yaitu : `total_acidity`, `acid_tosugar_ratio`, dan `density_alcohol_interaction`. dan kemudian di akhir di tampilkan sisa variabel fitur yang berjumlah 13

setelah di lakukan pengahpusan variabel berkorelasi tinggi selanjutnya akan di lakukan pemilhan fitur-fitur yang memiliki pengaruh besar pada model. untuk mendapatkan fitur-fitur yang memiliki pengaruh besar pada model di gunakan `random forest`, hal ini di karenakan random forest yang berbasis pohon keputusan akan secara alami menghitung variabel penting berdasarkan seberapa besar fitur tersebut dapat membantu membagi data dalam pohon.

pada kode di bawa mode random forest di latih dengan parameter 100 pohon keputusan
"""

rf_selector = RandomForestClassifier(n_estimators=100, random_state=42)
rf_selector.fit(X_reduced, y)

"""setelah model di latih, fitur-fitur yang berkontribusi tinggi akan di panggil dengan menggunakan kode `rf_selector.feature_importances_` yang kemudian hasilnya di simpan pada dataframe `feature_importance`"""

feature_importance = pd.DataFrame({
    'feature': X_reduced.columns,
    'importance': rf_selector.feature_importances_
}).sort_values('importance', ascending=False)

"""hasil dari dataframe yang telah di buat kemudian di cetak namun hanya di tampilkan 10 fetaure teratas"""

print(f"\nTop 10 most important features:")
print(feature_importance.head(10))

"""output di atas menunjukkan 10 feature teratas yang memberikan kontribusi besar pada model random forest."""

top_features = feature_importance.head(10)['feature'].tolist()
X_selected = X_reduced[top_features]

print(f"Final feature set: {top_features}")

"""hasil dari 10 feature teratas ini kemudian di buat variabel baru bernama X_selected, nah variabel ini lah yang akan di gunakan untuk melatih model.

### **Handling Imbalance Class**

imbalance class adalah ketika distribusi kelas target yang tidak seimbang. sebelum lanjut ke tahap splitting kita akan memastikan keseimbangan kelas target yaitu quality. untuk mrlihat distribusi kelas pada vairabel kuality makan kita akan menggunakan bar chart dengan menggunakan library matplotlib
"""

# Visualize target distribution
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
y.value_counts().sort_index().plot(kind='bar')
plt.title('Distribution of Wine Quality')
plt.xlabel('Quality Score')
plt.ylabel('Count')
plt.xticks(rotation=0)

plt.tight_layout()
plt.show()

"""hasil dari bar chart di atas menunjukkan perbedaan jumlah kelas yang sanagt signifikan, dimana terdapat 7 kelas dari 3 - 9 dan jarak perbedaan jumlahnya sangat terlampau jauh, kelas di dominasi oleh kelas 6 dengan jumlah lebih dari 2000

untuk menangani imbalance class ini ada berbagai metode mulai dari random oversampling, random undersampling, SMOTE (Synthetic Minority Over-Sampling techniqu). pada kode ini kita akan menggunakan SMOTE, smote di pilih karena ketimpangan yang sangat terlampau jauh antar kelas.

cara kerja SMOTE sendiri adalah memilih data minoritas secara acak, mencari tetangga terdekatnya, lalu membuat data baru di antara titik tersebut dengan interpolasi. Proses ini diulang sampai kelas minoritas memiliki jumlah data yang seimbang dengan kelas mayoritas. Dengan cara ini, SMOTE membantu model belajar pola kelas minoritas lebih baik tanpa hanya menyalin data lama, sehingga mengurangi risiko overfitting.

pada kode di bawah kita di hitung terlebih dahulu kondisi awal jumlah data dari masing-masing kelas, kemudian menginisialisasikan SMOTE dengan jumlah tetangga terdekatnya 3.  Setelah itu, data fitur (`X_selected`) dan target (`y`) diolah oleh SMOTE untuk membuat sampel sintetis pada kelas minoritas sehingga distribusi kelas menjadi seimbang. Hasilnya disimpan pada `X_balanced` dan `y_balanced`. Kemudian, kode mencetak distribusi kelas baru setelah penerapan SMOTE dan menampilkan ukuran dataset yang sudah seimbang.
"""

# Check class distribution
class_distribution = y.value_counts().sort_index()
print(f"Original class distribution:\n{class_distribution}")

# Apply SMOTE to handle imbalance
smote = SMOTE(random_state=42, k_neighbors=3)
X_balanced, y_balanced = smote.fit_resample(X_selected, y)

print(f"Balanced class distribution:\n{pd.Series(y_balanced).value_counts().sort_index()}")
print(f"Dataset shape after SMOTE: {X_balanced.shape}")

"""output di atas menunjukkan jumlah distribusi kelas sebelum dan sesudah proses SMOTE, dari yang awalnya tidak seimbang menjadi berjumlah sama 2032

kemudian agar data kelas hasi SMOTE dapat terlihat jelas di lakukan visualisasi bar chart pada variabel y_balanced.
"""

# Visualize target distribution
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
y_balanced.value_counts().sort_index().plot(kind='bar')
plt.title('Distribution of Wine Quality')
plt.xlabel('Quality Score')
plt.ylabel('Count')
plt.xticks(rotation=0)

plt.tight_layout()
plt.show()

"""dari bar chart di atas sudah terlihat jelas keseimbangan jumlah data dari semua kelas pada variabe y_balanced

### **Splitting Data**

sekarang masuk pada tahap splitting data atau membagi data menjadi data train dan test. data train di gunakan untuk proses pelatihan model sementara data test di gunakan untuk menguji coba performa akurasi dari model nantinya. pada kode ini data di bagi dengan cara 80% data pelatihan dan 20% data test
"""

# Split the data
X_train, X_test, y_train, y_test = train_test_split(
    X_balanced, y_balanced, test_size=0.2, random_state=42, stratify=y_balanced
)

print(f"Training set shape: {X_train.shape}")
print(f"Test set shape: {X_test.shape}")

"""output di atas menunjukkan jumlah data latih adalah 11379 sementara data test ada 2845, 10 itu merupakan jumlah fitur atau label

### **Feature Scalling**

Feature scaling adalah proses mengubah rentang nilai fitur agar berada dalam skala yang sama, biasanya dengan mengubah data ke distribusi yang memiliki rata-rata nol dan standar deviasi satu (standarisasi). proses ini sangat penting karena dengan ditur ini akan memastikan semua fitur memiliki bobot yang seimbang, dan juga nantinya akan meningkatkan konvergensi dari algoritma.

pada code ini digunakan fungsi standarScaler() untuk standarisasi fitur
"""

# Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Convert back to DataFrame for easier handling
X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)
X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)

"""## **Modeling**

pada tahap ini akan di pilih model model yang sesuai dan juga akan di setting hyper parameter yang sesuai untuk mendapatkan akurasi model yang tinggi

### **Model Selection and Hyperparameter**

Pada tahap pemodelan, digunakan lima algoritma klasifikasi populer: Random Forest, Support Vector Machine (SVM), K-Nearest Neighbors (KNN), XGBoost, dan Gradient Boosting. Masing-masing model dilatih dan dievaluasi dengan menggunakan **Randomized Search** untuk mencari kombinasi hyperparameter terbaik secara efisien.

Berikut rentang hyperparameter yang diuji:

1. `Random Forest:` jumlah pohon (n_estimators) 100-200, kedalaman pohon (max_depth) 10 atau tanpa batas, serta parameter min_samples_split, min_samples_leaf, dan max_features sesuai opsi yang ditentukan.

2. `SVM:` parameter regularisasi C 1 dan 10, kernel rbf, dan gamma dengan opsi scale dan 0.01.

3. `KNN:` tetangga terdekat (n_neighbors) 5 dan 9, bobot uniform, dan metrik euclidean.

4. `XGBoost:` jumlah pohon 100-200, kedalaman pohon 3 dan 6, learning rate 0.1, serta subsample dan colsample_bytree 0.9.

5. `Gradient Boosting:` jumlah pohon 100-200, kedalaman pohon 3 dan 5, learning rate 0.1, dan subsample 0.9.
"""

models = {
    'Random Forest': {
        'model': RandomForestClassifier(random_state=42),
        'params': {
            'n_estimators': [100, 200],
            'max_depth': [10, None],
            'min_samples_split': [2, 5],
            'min_samples_leaf': [1, 2],
            'max_features': ['sqrt']
        }
    },
    'SVM': {
        'model': SVC(random_state=42),
        'params': {
            'C': [1, 10],
            'kernel': ['rbf'],
            'gamma': ['scale', 0.01]
        }
    },
    'KNN': {
        'model': KNeighborsClassifier(),
        'params': {
            'n_neighbors': [5, 9],
            'weights': ['uniform'],
            'metric': ['euclidean']
        }
    },
    'XGBoost': {
        'model': XGBClassifier(random_state=42, eval_metric='mlogloss'),
        'params': {
            'n_estimators': [100, 200],
            'max_depth': [3, 6],
            'learning_rate': [0.1],
            'subsample': [0.9],
            'colsample_bytree': [0.9]
        }
    },
    'Gradient Boosting': {
        'model': GradientBoostingClassifier(random_state=42),
        'params': {
            'n_estimators': [100, 200],
            'max_depth': [3, 5],
            'learning_rate': [0.1],
            'subsample': [0.9]
        }
    }
}

# Train and evaluate models
results = {}
best_models = {}

"""### **Model Training**

pada tahap ini masing-masing model dengan hyper paramter yang telah di set akan di latih satu persatu dengan rentang parameter yang sudah di tentukan untuk mendapatkan parameter terbaaik dari masing-masing model
"""

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

"""Kode ini membuat metode validasi silang dengan membagi data menjadi 5 bagian (fold) secara seimbang berdasarkan kelas target (stratified). Data diacak dulu sebelum dibagi agar pembagian lebih merata, dan pengacakan ini dapat diulang sama persis karena menggunakan `random_state=42`. Teknik ini membantu memastikan evaluasi model yang lebih adil dan representatif.

Kode di bawah ini  melakukan pelatihan dan pencarian hyperparameter terbaik untuk beberapa model menggunakan RandomizedSearchCV dengan validasi silang. Label target di-encode untuk XGBoost agar kompatibel. Setelah pelatihan, kode menghitung akurasi, F1-score, dan skor validasi silang, kemudian menyimpan model terbaik beserta hasil evaluasi. Hasil terbaik dan metrik performa dicetak untuk setiap model guna memudahkan pemilihan model yang optimal.
"""

le = LabelEncoder()

y_train_encoded = le.fit_transform(y_train)
y_test_encoded = le.transform(y_test)

best_models = {}
results = {}

for name, model_info in models.items():
    print(f"\nTraining {name}...")

    random_search = RandomizedSearchCV(
        estimator=model_info['model'],
        param_distributions=model_info['params'],
        n_iter=20,
        cv=cv,
        scoring='accuracy',
        n_jobs=-1,
        verbose=1,
        random_state=42
    )

    if name == 'XGBoost':
        random_search.fit(X_train_scaled, y_train_encoded)
        best_model = random_search.best_estimator_
        y_pred = best_model.predict(X_test_scaled)
        accuracy = accuracy_score(y_test_encoded, y_pred)
        f1 = f1_score(y_test_encoded, y_pred, average='weighted')
        cv_scores = cross_val_score(best_model, X_train_scaled, y_train_encoded, cv=cv, scoring='accuracy')
    else:
        random_search.fit(X_train_scaled, y_train)
        best_model = random_search.best_estimator_
        y_pred = best_model.predict(X_test_scaled)
        accuracy = accuracy_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred, average='weighted')
        cv_scores = cross_val_score(best_model, X_train_scaled, y_train, cv=cv, scoring='accuracy')

    best_models[name] = best_model
    results[name] = {
        'best_params': random_search.best_params_,
        'test_accuracy': accuracy,
        'f1_score': f1,
        'cv_mean': cv_scores.mean(),
        'cv_std': cv_scores.std(),
        'predictions': y_pred
    }

    print(f"Best parameters: {random_search.best_params_}")
    print(f"Test Accuracy: {accuracy:.4f}")
    print(f"F1 Score: {f1:.4f}")
    print(f"CV Score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")

"""Output menunjukkan proses pelatihan dan pencarian hyperparameter terbaik untuk lima model klasifikasi menggunakan validasi silang 5-fold. Setiap model diuji dengan sejumlah kombinasi parameter, dan hasil terbaik disajikan dengan metrik evaluasi pada data uji dan validasi silang.

* **Random Forest** menunjukkan performa terbaik dengan akurasi tes 85.69% dan F1-score 85.18%. Hyperparameter optimal termasuk 200 pohon, tanpa batas kedalaman (`max_depth=None`), dan fitur yang dipilih menggunakan metode `'sqrt'`.

* **XGBoost** dan **Gradient Boosting** juga memberikan hasil yang sangat baik dengan akurasi sekitar 84% dan F1-score mendekati 83-84%, menunjukkan kemampuan boosting dalam menangani data ini.

* **SVM** dan **KNN** memiliki performa sedikit lebih rendah, dengan akurasi sekitar 78-79% dan F1-score sekitar 76-78%. SVM optimal menggunakan kernel RBF dengan `C=10` dan gamma `'scale'`, sedangkan KNN optimal menggunakan 5 tetangga dengan bobot uniform dan metrik Euclidean.

Nilai rata-rata akurasi validasi silang (`CV Score`) mendekati nilai akurasi tes, dengan standar deviasi kecil, menandakan model stabil dan tidak overfitting.

## **Evaluation**

setelah selesai melakukan pelatihan model dan mendapatkan nilai akurasi, f1, mean dan std dev maka selanjutnya akan di lakukan perbandingan lagi model mana yang paling stabil dan memiliki akurasi tinggi

pada kode di bawah ini di buat sebuah list kosong yang akan berisi 5 kolom yaitu model, test Accuracy, F1 Score, CV Mean Accuracy, dan CV std Dev. nah setiap model kemudian akan urutkan berdasarkan 5 nilai tertinggi tadi dalam dataframe
"""

# Buat list untuk menyimpan ringkasan hasil evaluasi
summary_list = []

for model_name, metrics in results.items():
    summary_list.append({
        'Model': model_name,
        'Test Accuracy': metrics.get('test_accuracy', None),
        'F1 Score': metrics.get('f1_score', None),
        'CV Mean Accuracy': metrics.get('cv_mean', None),
        'CV Std Dev': metrics.get('cv_std', None)
    })

# Buat dataframe dari list
summary_df = pd.DataFrame(summary_list)

# Urutkan berdasarkan Test Accuracy dari terkecil ke terbesar
summary_df = summary_df.sort_values(by='Test Accuracy', ascending=False).reset_index(drop=True)

print(summary_df)

"""dari output di atas maka bisa di ketahui bawha model terbaik adalah **Random Forest** karena:
- Memiliki akurasi uji tertinggi (85.69%)
- F1 Score terbaik (0.8518), menunjukkan keseimbangan antara precision dan recall
- Cross-validation menunjukkan hasil yang stabil dengan standar deviasi kecil (0.0052)

agar data hasil akurasi lebih mudah di baca maka pada kode di bawah di sebuah bar plot horizontal.
"""

# Urutkan data berdasarkan akurasi dari terkecil ke terbesar
summary_df_sorted = summary_df.sort_values(by='Test Accuracy', ascending=True)

plt.figure(figsize=(10,6))
sns.barplot(x='Test Accuracy', y='Model', data=summary_df_sorted, palette='viridis')

plt.title('Perbandingan Akurasi Model')
plt.xlabel('Test Accuracy')
plt.ylabel('Model')
plt.xlim(0, 1)  # rentang akurasi dari 0 sampai 1
plt.tight_layout()
plt.show()

"""dari bar plot di atas maka di ketahui akurasi tertinggi adalah random forest kemudian di bawahnya ada XGBoost dan Gradient Boosting"""