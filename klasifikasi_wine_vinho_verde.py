# -*- coding: utf-8 -*-
"""Klasifikasi_wine_vinho_verde.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ri-eXU0z08tDIHRxffL6fVIRIykg9mBf

Laporan Proyek Machine Learning - Moh. Novil Ma'arij

Domain Proyek : Pemasaran/Ekonomi

# **Klasifikasi kualitas wine Vinho Verde**

![gambar wine vinho verde](https://static.independent.co.uk/2023/07/13/11/vintages-buy-43784e0e-11dc-11ee-9aaf-3ad6dbe6fd05.jpg?quality=75&width=1250&crop=3%3A2%2Csmart&auto=webp)

## **Latar Belakang**

Region Vinho Verde di Portugal telah lama dikenal sebagai produsen anggur yang unik dengan karakter rasa yang segar, asam, dan sedikit bergelembung (frizzante). Terdapat dua jenis wine Vinho Verde, yaitu white dan red, yang masing-masing memiliki varietas tersendiri.

Seiring dengan perubahan pola pasar dan ketatnya persaingan, konsumen kini tidak hanya mencari wine yang enak, tetapi juga wine dengan kualitas terbaik.

Menyikapi hal ini, produsen wine Vinho Verde selama ini melakukan klasifikasi kualitas wine secara tradisional, seperti menggunakan metode sensorik dan teknik kimiawi. Kedua metode tersebut memberikan hasil yang baik, namun memiliki kelemahan yaitu tingkat human error yang cukup tinggi.

Oleh karena itu, dalam proyek ini akan dikembangkan sebuah model machine learning untuk mengklasifikasikan kualitas wine Vinho Verde berdasarkan kadar kimiawinya.

## **Import Library**
"""

# Import libraries
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

# Machine Learning libraries
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, f1_score
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.combine import SMOTETomek
from sklearn.model_selection import RandomizedSearchCV
from sklearn.preprocessing import LabelEncoder

"""# **Data Understanding**

### **Data Loading**
"""

# files.upload()

# !mkdir -p ~/.kaggle
# !cp kaggle.json ~/.kaggle/
# !chmod 600 ~/.kaggle/kaggle.json

# !kaggle datasets download -d rajyellow46/wine-quality
# !unzip wine-quality.zip

vinho_verde = pd.read_csv('winequalityN.csv')

"""### **Dataset Informations**

| Jenis | Keterangan |
|-------|------------|
|Nama   | Wine Quality|
|Sumber | [Kaggle](https://www.kaggle.com/datasets/rajyellow46/wine-quality)|
|Pengelola| [Raj Parmar](https://www.kaggle.com/rajyellow46)
|Lisensi| Tidak Spesifik|
|Visibilitas| Publik|
|Label| *Earth and Nature, Classification, Alcohol, Regression*|
|Kebergunaan| 7.6|

### **Variabel Description**
"""

vinho_verde.sample(3)

vinho_verde.info()

vinho_verde.describe()

"""### **Univariate Analysis**"""

numerical_cols = vinho_verde.select_dtypes(include=[np.number]).columns

n = len(numerical_cols)
cols = 2  # jumlah kolom
rows = (n + cols - 1) // cols  # hitung baris supaya cukup

fig, axes = plt.subplots(rows, cols, figsize=(12, rows * 3))
axes = axes.flatten()  # supaya mudah indexing

for i, col in enumerate(numerical_cols):
    axes[i].hist(vinho_verde[col].dropna(), bins=30, edgecolor='black')
    axes[i].set_title(f'Distribusi Histogram: {col}')
    axes[i].set_xlabel(col)
    axes[i].set_ylabel('Frekuensi')

# Hilangkan axes kosong jika ada
for j in range(i+1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

"""### **Multivariate Analysis**"""

corr = vinho_verde[numerical_cols].corr()

# Visualisasi heatmap korelasi
plt.figure(figsize=(12,8))
sns.heatmap(corr, annot=True, fmt=".2f", cmap='coolwarm', square=True)
plt.title('Heatmap Korelasi Variabel Numerik')
plt.show()

"""## **Data Preparation**

### **Remove Missing Values**
"""

vinho_verde.isnull().sum()

vinho_verde = vinho_verde.dropna()

vinho_verde.isnull().sum()

"""### **Remove Duplicates Values**"""

vinho_verde.duplicated().sum()

vinho_verde.drop_duplicates(inplace=True)

vinho_verde.duplicated().sum()

"""### **Find Multicollinearity**"""

def find_correlated_features(corr_matrix, threshold=0.8):
    """Find pairs of highly correlated features"""
    high_corr_pairs = []
    for i in range(len(corr_matrix.columns)):
        for j in range(i+1, len(corr_matrix.columns)):
            if abs(corr_matrix.iloc[i, j]) > threshold:
                high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))
    return high_corr_pairs

corr_matrix = vinho_verde[numerical_cols].corr()
high_corr = find_correlated_features(corr_matrix, 0.7)
print(f"\nHighly correlated feature pairs (>0.7):")
for pair in high_corr:
    print(f"{pair[0]} - {pair[1]}: {pair[2]:.3f}")

"""### **Feature Engineering**"""

# 1. Encode categorical variables
vinho_verde_processed = vinho_verde.copy()
vinho_verde_processed['type_encoded'] = vinho_verde_processed['type'].map({'white': 0, 'red': 1})

# 2. Create new features
vinho_verde_processed['total_acidity'] = vinho_verde_processed['fixed acidity'] + vinho_verde_processed['volatile acidity']
vinho_verde_processed['acid_to_sugar_ratio'] = vinho_verde_processed['total_acidity'] / (vinho_verde_processed['residual sugar'] + 1)
vinho_verde_processed['so2_ratio'] = vinho_verde_processed['free sulfur dioxide'] / (vinho_verde_processed['total sulfur dioxide'] + 1)
vinho_verde_processed['density_alcohol_interaction'] = vinho_verde_processed['density'] * vinho_verde_processed['alcohol']

"""### **Remove Outliers**"""

plt.figure(figsize=(15, 12))
for i, col in enumerate(numerical_cols, 1):
    plt.subplot(4, 3, i)
    sns.boxplot(y=vinho_verde[col])
    plt.title(f'Boxplot {col}')
plt.tight_layout()
plt.show()

def remove_outliers_iqr(df, columns, factor=2.0):  # Using factor=2.0 instead of 1.5 to be less aggressive
    """Remove outliers using IQR method"""
    df_clean = df.copy()

    for col in columns:
        Q1 = df_clean[col].quantile(0.25)
        Q3 = df_clean[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - factor * IQR
        upper_bound = Q3 + factor * IQR

        before_count = len(df_clean)
        df_clean = df_clean[(df_clean[col] >= lower_bound) & (df_clean[col] <= upper_bound)]
        after_count = len(df_clean)
        print(f"Removed {before_count - after_count} outliers from {col}")

    return df_clean

numerical_features = [col for col in numerical_cols if col != 'quality']
print(f"Shape before outlier removal: {vinho_verde_processed.shape}")
vinho_verde_processed = remove_outliers_iqr(vinho_verde_processed, numerical_features)
print(f"Shape after outlier removal: {vinho_verde_processed.shape}")

"""### **Remove High Correlation Feature**"""

# Prepare features and target
feature_columns = [col for col in vinho_verde_processed.columns if col not in ['quality', 'type']]
X = vinho_verde_processed[feature_columns]
y = vinho_verde_processed['quality']

print(f"Features used: {feature_columns}")

def remove_high_correlation(X, threshold=0.8):
    """Remove highly correlated features"""
    corr_matrix = X.corr().abs()
    upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))

    to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > threshold)]
    print(f"Dropping highly correlated features: {to_drop}")

    return X.drop(columns=to_drop)

X_reduced = remove_high_correlation(X, threshold=0.8)
print(f"Features after correlation removal: {X_reduced.shape[1]}")

rf_selector = RandomForestClassifier(n_estimators=100, random_state=42)
rf_selector.fit(X_reduced, y)

feature_importance = pd.DataFrame({
    'feature': X_reduced.columns,
    'importance': rf_selector.feature_importances_
}).sort_values('importance', ascending=False)

print(f"\nTop 10 most important features:")
print(feature_importance.head(10))

top_features = feature_importance.head(10)['feature'].tolist()
X_selected = X_reduced[top_features]

print(f"Final feature set: {top_features}")

"""### **Handling Imbalance Class**"""

# Visualize target distribution
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
y.value_counts().sort_index().plot(kind='bar')
plt.title('Distribution of Wine Quality')
plt.xlabel('Quality Score')
plt.ylabel('Count')
plt.xticks(rotation=0)

plt.tight_layout()
plt.show()

# Check class distribution
class_distribution = y.value_counts().sort_index()
print(f"Original class distribution:\n{class_distribution}")

# Apply SMOTE to handle imbalance
smote = SMOTE(random_state=42, k_neighbors=3)
X_balanced, y_balanced = smote.fit_resample(X_selected, y)

print(f"Balanced class distribution:\n{pd.Series(y_balanced).value_counts().sort_index()}")
print(f"Dataset shape after SMOTE: {X_balanced.shape}")

# Visualize target distribution
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
y_balanced.value_counts().sort_index().plot(kind='bar')
plt.title('Distribution of Wine Quality')
plt.xlabel('Quality Score')
plt.ylabel('Count')
plt.xticks(rotation=0)

plt.tight_layout()
plt.show()

"""### **Splitting Data**"""

# Split the data
X_train, X_test, y_train, y_test = train_test_split(
    X_balanced, y_balanced, test_size=0.2, random_state=42, stratify=y_balanced
)

print(f"Training set shape: {X_train.shape}")
print(f"Test set shape: {X_test.shape}")

"""### **Feature Scalling**"""

# Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Convert back to DataFrame for easier handling
X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)
X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)

"""## **Modeling**

### **Model Selection and Hyperparameter**
"""

models = {
    'Random Forest': {
        'model': RandomForestClassifier(random_state=42),
        'params': {
            'n_estimators': [100, 200],
            'max_depth': [10, None],
            'min_samples_split': [2, 5],
            'min_samples_leaf': [1, 2],
            'max_features': ['sqrt']
        }
    },
    'SVM': {
        'model': SVC(random_state=42),
        'params': {
            'C': [1, 10],
            'kernel': ['rbf'],
            'gamma': ['scale', 0.01]
        }
    },
    'KNN': {
        'model': KNeighborsClassifier(),
        'params': {
            'n_neighbors': [5, 9],
            'weights': ['uniform'],
            'metric': ['euclidean']
        }
    },
    'XGBoost': {
        'model': XGBClassifier(random_state=42, eval_metric='mlogloss'),
        'params': {
            'n_estimators': [100, 200],
            'max_depth': [3, 6],
            'learning_rate': [0.1],
            'subsample': [0.9],
            'colsample_bytree': [0.9]
        }
    },
    'Gradient Boosting': {
        'model': GradientBoostingClassifier(random_state=42),
        'params': {
            'n_estimators': [100, 200],
            'max_depth': [3, 5],
            'learning_rate': [0.1],
            'subsample': [0.9]
        }
    }
}

# Train and evaluate models
results = {}
best_models = {}

"""### **Model Training**"""

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

le = LabelEncoder()

y_train_encoded = le.fit_transform(y_train)
y_test_encoded = le.transform(y_test)

best_models = {}
results = {}

for name, model_info in models.items():
    print(f"\nTraining {name}...")

    random_search = RandomizedSearchCV(
        estimator=model_info['model'],
        param_distributions=model_info['params'],
        n_iter=20,
        cv=cv,
        scoring='accuracy',
        n_jobs=-1,
        verbose=1,
        random_state=42
    )

    if name == 'XGBoost':
        random_search.fit(X_train_scaled, y_train_encoded)
        best_model = random_search.best_estimator_
        y_pred = best_model.predict(X_test_scaled)
        accuracy = accuracy_score(y_test_encoded, y_pred)
        f1 = f1_score(y_test_encoded, y_pred, average='weighted')
        cv_scores = cross_val_score(best_model, X_train_scaled, y_train_encoded, cv=cv, scoring='accuracy')
    else:
        random_search.fit(X_train_scaled, y_train)
        best_model = random_search.best_estimator_
        y_pred = best_model.predict(X_test_scaled)
        accuracy = accuracy_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred, average='weighted')
        cv_scores = cross_val_score(best_model, X_train_scaled, y_train, cv=cv, scoring='accuracy')

    best_models[name] = best_model
    results[name] = {
        'best_params': random_search.best_params_,
        'test_accuracy': accuracy,
        'f1_score': f1,
        'cv_mean': cv_scores.mean(),
        'cv_std': cv_scores.std(),
        'predictions': y_pred
    }

    print(f"Best parameters: {random_search.best_params_}")
    print(f"Test Accuracy: {accuracy:.4f}")
    print(f"F1 Score: {f1:.4f}")
    print(f"CV Score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")

"""## **Evaluation**"""

# Buat list untuk menyimpan ringkasan hasil evaluasi
summary_list = []

for model_name, metrics in results.items():
    summary_list.append({
        'Model': model_name,
        'Test Accuracy': metrics.get('test_accuracy', None),
        'F1 Score': metrics.get('f1_score', None),
        'CV Mean Accuracy': metrics.get('cv_mean', None),
        'CV Std Dev': metrics.get('cv_std', None)
    })

# Buat dataframe dari list
summary_df = pd.DataFrame(summary_list)

# Urutkan berdasarkan Test Accuracy dari terkecil ke terbesar
summary_df = summary_df.sort_values(by='Test Accuracy', ascending=False).reset_index(drop=True)

print(summary_df)

# Urutkan data berdasarkan akurasi dari besar ke keci
summary_df_sorted = summary_df.sort_values(by='Test Accuracy', ascending=True)

plt.figure(figsize=(10,6))
sns.barplot(x='Test Accuracy', y='Model', data=summary_df_sorted, palette='viridis')

plt.title('Perbandingan Akurasi Model')
plt.xlabel('Test Accuracy')
plt.ylabel('Model')
plt.xlim(0, 1)  # rentang akurasi dari 0 sampai 1
plt.tight_layout()
plt.show()